---
title: "Assignment 1"
---

# Assignment 1

```{r}
1 + 1
```

My first assignment has two parts.

## (a)

I choosed the video of Mustafa Gokce Baydogan, and here is my summary:

Mustafa Gokce Baydogan, an industrial engineer and founder of Algopoly, shared insights about his career and his company. He discussed the connection between optimization, modeling, and data science, emphasizing their importance for industrial engineering. Before building complex models, he highlighted the need to analyze the data.

Baydogan used examples, such as a shoe sales forecasting problem, to illustrate data-related challenges. He also discussed a 2019 project focused on predicting wood warping during drying, a significant issue as warped wood (bowed or crooked) sells for a lower price, causing financial losses. The goal was to produce straight wood by taking preventive measures during drying. He explained how he researched the causes of warping by consulting experts. Then by applying filters to wood images and using mathematical calculations, data from the visuals is extracted. The project utilized linear regression. The importance of using images in data analysis was also emphasized. By fixing calculated points, the production of straight wood was achieved.

He explained the differences between traditional machine learning and deep learning, noting that industrial engineering students have a good understanding of feature extraction. He introduced the concepts of interpretability, steerability, robustness, and decision-making in feature engineering.

An example from the electricity market was also given. He talked about the need to balance production and consumption, where inaccurate forecasts could lead to significant costs. He discussed the impact of seasonality and special events on decision-making.

Baydogan also mentioned Trendyol's ranking algorithms and how recommendation models are designed to drive sales, as well as the use of Google Trends. He addressed biased data and talked about reinforcement learning.

In discussing new frontiers, he introduced physics-informed machine learning and dynamic systems and the use of machine learning for optimization, exploring whether machine learning could be leveraged to improve optimization techniques.

***Question 1***

1.  interpretability (regulation)
2.  steerability
3.  robustness
4.  decision-making

Which one/ones did he mentioned when he was talking about traditional machine learning and deep learning differences?

a\) 1 and 2 b) 1 and 3 c) 1, 2 and 3 *d) All of them*

***Question 2***

Why do we need human sources? Why he doesn't think that ai won't be able to take all of our jobs at least in the next a few years?

*Understanding systems and quantifying them still requires human involvement, and in fact, much of the data is messy. There is still a great need for human expertise to obtain accurate data and perform correct transformations.*

*The need to develop alternative approaches for structured and unstructured data is increasing.*

*Human guidance makes a difference.*

*Interpretability remains crucial.*

## (b)

First, "polls_us_election_2016" data set is imported by the codes shown below from the dslabs package.

```{r}
library(dslabs)
data(polls_us_election_2016)
```

To display the first 10 rows of the data set, we may use **head()** function. **head()** function shows the first lines (rows) of the data frame, and we can choose the number of rows we want to see.

```{r}
head(polls_us_election_2016, 10)
```

Next, total number of NA values in the entire data set is wanted to be calculated. If we want to calculate the total number of something we use **sum()** function. For this, **is.na()** will help us, too.

```{r}
total_number_na <- sum(is.na(polls_us_election_2016))
print(total_number_na)
```

Now, it's wanted to replace all NA values:

-   with birth year for numeric columns,

-   with first name for character or factor columns.

Also, we will store this new data set as a new object.

*P.S. In here, i've used ai for the factor part because i was having some errors and couldn't figure out what to do.*

```{r}
new_data_set <- polls_us_election_2016
birth_year <- 2004
first_name <- "Cerem"

for (x in names(new_data_set)) {
  if (is.numeric(new_data_set[[x]])) {
    #if the column is numeric
    new_data_set[[x]][is.na(new_data_set[[x]])] <- birth_year
     #replace NA with birth year
  }
 else if (is.character(new_data_set[[x]])) {
    #if the column is character
    new_data_set[[x]][is.na(new_data_set[[x]])] <- first_name
     #replace NA with first name
 }
  else if (is.factor(new_data_set[[x]])) {
    #if the column is factor
    levels(new_data_set[[x]]) <- c(levels(new_data_set[[x]]), first_name)
    #adding the first_name to the factor levels
    new_data_set[[x]][is.na(new_data_set[[x]])] <- first_name
    #replace NA with first name
  }
}
```

Finally, we'll print the first 10 rows of the new data frame and the total number of NAs remaining in the new data frame. To do this, we'll use **head()** and **sum()** functions again.

```{r}
head(new_data_set, 10)

print(new_total_number_na <- sum(is.na(new_data_set)))
```
